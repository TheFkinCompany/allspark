{
    "docs": [
        {
            "location": "/", 
            "text": "Allspark", 
            "title": "Index"
        }, 
        {
            "location": "/#allspark", 
            "text": "", 
            "title": "Allspark"
        }, 
        {
            "location": "/install/", 
            "text": "Installation guide\n\n\nRequirements\n\n\nOn the allspark machine\n\n\n\n\nDocker ( tested with version 18.04 )\n\n\n\n\nOn the control machine\n\n\n\n\nAnsible ( tested with version 2.5.5 )\n\n\n\n\nAnsible playbook\n\n\n\n\nEdit the file \ngroup_vars/all.yml\n to fit with\nyour needs, like:\n\n\nThe \nallspark_root_domain\n to use your domain name\n    (each component will be exposed as a subdomain).\n\n\nEnable or disable component using their \nenabled\n boolean toggle\n\n\n\n\n\n\nNote\n\n\nYou can customize the image or tag for a component by overriding the \ncomponent_image\n and \ncomponent_tag\n, using either :\n\n\n\n\nAnsible extra vars\n\n\nAdd those variables to your \ngroup_vars/all.yml\n file.\n\n\n\n\ne.g\n: (group_vars)\n\n# Change gitlab-ce to gitlab-ee\n\n\ngitlab_image\n:\n \ngitlab_ee\n\n\ngitlab_tag\n:\n \nlatest\n\n\n\nYou can access the complete list of available components in the \nroles/downloads/defaults/main.yml\n file.\n\n\n\n\n\n\nWarning\n\n\nFor offline install, the images configuration must be the same on both end.\n\n\n\n\nIf you are on a migration see the \noperation pages\n\n\nIf you are on a migration see the \noperation pages\n\n\nOnline install\n\n\n\n\nChange the hosts file to point to the allspark machine.\n\n\n\n\nansible-playbook -i hosts install.yml\n\n\n\n\nAt this point, you will be able to see component show up along with their access URL\nat \ningress.YOUR_ROOT_DOMAIN\n.\n\n\nOffline install\n\n\n0. On the online control machine\n\n\n\n\nNote\n\n\nChange the \nallspark_release_destination\n and \nallspark_release_tmp_directory\n to point to somwhere with at least 10Go of free space\n\n\n\n\n\n\n\n\nBash\n\n\nansible-playbook -i hosts release.yml\n\n\n\n\n\nhosts\n\n\n[all]\n\n\nlocalhost ansible_connection\n=\nlocal\n\n\n\n\n\n\nit will generate a \n.tar.gz\n file at \nallspark_release_destination\n.\nThis file needs to be copied along with the \ngroup_vars\n\ndirectory to the offline allspark machine.\n\n\n1. On the offline allspark machine\n\n\n\n\nImport the \ngroup_vars\n directory and the relase \n.tar.gz\n\n\nSet the \nallspark_release_destination\n in the \ngroup_vars/all.yml\n file to point to the release.\n\n\nSet the \nallspark_release_tmp_directory\n to point to somwhere with at least 10Go of free space\n\n\n\n\n\n\n\n\nBash\n\n\nansible-playbook -i hosts setup.yml\nansible-playbook -i hosts install.yml\n\n\n\n\n\nhosts\n\n\n[all]\n\n\nlocalhost ansible_connection\n=\nlocal\n\n\n\n\n\n\n\n\nWarning\n\n\nThis playbook does not setup the system yet. The release is packaged along with\nCentOS RPM dependencies as well as pip wheels, but you will however have to install them yourself.", 
            "title": "Install"
        }, 
        {
            "location": "/install/#installation-guide", 
            "text": "", 
            "title": "Installation guide"
        }, 
        {
            "location": "/install/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/install/#on-the-allspark-machine", 
            "text": "Docker ( tested with version 18.04 )", 
            "title": "On the allspark machine"
        }, 
        {
            "location": "/install/#on-the-control-machine", 
            "text": "Ansible ( tested with version 2.5.5 )", 
            "title": "On the control machine"
        }, 
        {
            "location": "/install/#ansible-playbook", 
            "text": "Edit the file  group_vars/all.yml  to fit with\nyour needs, like:  The  allspark_root_domain  to use your domain name\n    (each component will be exposed as a subdomain).  Enable or disable component using their  enabled  boolean toggle    Note  You can customize the image or tag for a component by overriding the  component_image  and  component_tag , using either :   Ansible extra vars  Add those variables to your  group_vars/all.yml  file.   e.g : (group_vars) # Change gitlab-ce to gitlab-ee  gitlab_image :   gitlab_ee  gitlab_tag :   latest  \nYou can access the complete list of available components in the  roles/downloads/defaults/main.yml  file.    Warning  For offline install, the images configuration must be the same on both end.   If you are on a migration see the  operation pages  If you are on a migration see the  operation pages", 
            "title": "Ansible playbook"
        }, 
        {
            "location": "/install/#online-install", 
            "text": "Change the hosts file to point to the allspark machine.   ansible-playbook -i hosts install.yml  At this point, you will be able to see component show up along with their access URL\nat  ingress.YOUR_ROOT_DOMAIN .", 
            "title": "Online install"
        }, 
        {
            "location": "/install/#offline-install", 
            "text": "", 
            "title": "Offline install"
        }, 
        {
            "location": "/install/#0-on-the-online-control-machine", 
            "text": "Note  Change the  allspark_release_destination  and  allspark_release_tmp_directory  to point to somwhere with at least 10Go of free space     Bash  ansible-playbook -i hosts release.yml   hosts  [all]  localhost ansible_connection = local    it will generate a  .tar.gz  file at  allspark_release_destination .\nThis file needs to be copied along with the  group_vars \ndirectory to the offline allspark machine.", 
            "title": "0. On the online control machine"
        }, 
        {
            "location": "/install/#1-on-the-offline-allspark-machine", 
            "text": "Import the  group_vars  directory and the relase  .tar.gz  Set the  allspark_release_destination  in the  group_vars/all.yml  file to point to the release.  Set the  allspark_release_tmp_directory  to point to somwhere with at least 10Go of free space     Bash  ansible-playbook -i hosts setup.yml\nansible-playbook -i hosts install.yml   hosts  [all]  localhost ansible_connection = local     Warning  This playbook does not setup the system yet. The release is packaged along with\nCentOS RPM dependencies as well as pip wheels, but you will however have to install them yourself.", 
            "title": "1. On the offline allspark machine"
        }, 
        {
            "location": "/roadmap/", 
            "text": "Roadmap", 
            "title": "Roadmap"
        }, 
        {
            "location": "/roadmap/#roadmap", 
            "text": "", 
            "title": "Roadmap"
        }, 
        {
            "location": "/architecture/", 
            "text": "Architecture\n\n\nComponents\n\n\n\n\n\n\n\n\nComponents\n\n\nUsage\n\n\n\n\n\n\n\n\n\n\nGitlab\n\n\nSource code management\n\n\n\n\n\n\n\n\nContinuous integration / delivery\n\n\n\n\n\n\n\n\nWork planning\n\n\n\n\n\n\n\n\nIssue tracking\n\n\n\n\n\n\nPortainer\n\n\nContainer management\n\n\n\n\n\n\nRocketChat\n\n\nInstant chat (Slack like)\n\n\n\n\n\n\nGrafana\n\n\nMonitoring\n\n\n\n\n\n\nPrometheus\n\n\nMetrics scraping / store\n\n\n\n\n\n\nCAdvisor\n\n\nContainer metrics\n\n\n\n\n\n\nNode exporter\n\n\nSystem metrics\n\n\n\n\n\n\nSonarqube\n\n\nContinuous Inspection\n\n\n\n\n\n\nTraefik\n\n\nDynamic reverse proxy\n\n\n\n\n\n\n\n\nPlaybooks\n\n\n\n\nrelease.yml\n\n\n\n\n\n\nPackage an Allspark release in a \n.tar.gz\n file\n\n\n\n\n\n\nsetup.yml\n\n\n\n\n\n\nLoad a packaged Allspark release into the system\n\n\n\n\n\n\ninstall.yml\n\n\n\n\n\n\nInstall Allspark on the machine (either with an internet access or after using \nsetup.yml\n playbook)", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#components", 
            "text": "Components  Usage      Gitlab  Source code management     Continuous integration / delivery     Work planning     Issue tracking    Portainer  Container management    RocketChat  Instant chat (Slack like)    Grafana  Monitoring    Prometheus  Metrics scraping / store    CAdvisor  Container metrics    Node exporter  System metrics    Sonarqube  Continuous Inspection    Traefik  Dynamic reverse proxy", 
            "title": "Components"
        }, 
        {
            "location": "/architecture/#playbooks", 
            "text": "release.yml    Package an Allspark release in a  .tar.gz  file    setup.yml    Load a packaged Allspark release into the system    install.yml    Install Allspark on the machine (either with an internet access or after using  setup.yml  playbook)", 
            "title": "Playbooks"
        }, 
        {
            "location": "/operation/", 
            "text": "Warning\n\n\nPay attention when you launch the \nbackup.yml\n or \nrestore.yml\n because playbook automatically \nstop and start\n the container to \nkeep all the data safe\n.\n\n\n\n\nBackup guide\n\n\nRequirements\n\n\nOn the allspark machine\n\n\n\n\nHave an allspark instance running\n\n\n\n\nAnsible Backup playbook\n\n\n\n\nLaunch the playbook to make your backup, like:\n\n\n\n\nansible-playbook -i hosts backup.yml\n\n\n\n\n\n\nNote\n\n\nYou can change the release destination (default: \n/opt/allspark/backup\n) like so:\n\nansible-playbook -i hosts backup.yml --extra-vars \nallspark_backup_directory=/tmp/allspark_backup\n\n\n\n\n\n\nRestore guide\n\n\nRequirements\n\n\nOn the allspark machine\n\n\n\n\nHave the same installation of your backup to be able to restore your release\n\n\n\n\n\n\nWarning\n\n\nPay attention:\n\n\n\n\nA component disabled (on the allspark machine) won't be imported on restore\n\n\nrestore needs a running instance\n\n\nPrevious datas of the restored containers will be lost (and aligned on the backup ones)\n\n\n\n\n\n\nAnsible Restore playbook\n\n\n\n\nLaunch the playbook Restore to make the restore\n\n\n\n\nansible-playbook -i hosts restore.yml\n\n\n\n\n\n\nNote\n\n\nYou can launch a dry-run mode if you want to test your backup to restore, like so:\n\nansible-playbook -i hosts restore.yml --extra-vars \nallspark_restore_dry_run=true", 
            "title": "Operation"
        }, 
        {
            "location": "/operation/#backup-guide", 
            "text": "", 
            "title": "Backup guide"
        }, 
        {
            "location": "/operation/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/operation/#on-the-allspark-machine", 
            "text": "Have an allspark instance running", 
            "title": "On the allspark machine"
        }, 
        {
            "location": "/operation/#ansible-backup-playbook", 
            "text": "Launch the playbook to make your backup, like:   ansible-playbook -i hosts backup.yml   Note  You can change the release destination (default:  /opt/allspark/backup ) like so: ansible-playbook -i hosts backup.yml --extra-vars  allspark_backup_directory=/tmp/allspark_backup", 
            "title": "Ansible Backup playbook"
        }, 
        {
            "location": "/operation/#restore-guide", 
            "text": "", 
            "title": "Restore guide"
        }, 
        {
            "location": "/operation/#requirements_1", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/operation/#on-the-allspark-machine_1", 
            "text": "Have the same installation of your backup to be able to restore your release    Warning  Pay attention:   A component disabled (on the allspark machine) won't be imported on restore  restore needs a running instance  Previous datas of the restored containers will be lost (and aligned on the backup ones)", 
            "title": "On the allspark machine"
        }, 
        {
            "location": "/operation/#ansible-restore-playbook", 
            "text": "Launch the playbook Restore to make the restore   ansible-playbook -i hosts restore.yml   Note  You can launch a dry-run mode if you want to test your backup to restore, like so: ansible-playbook -i hosts restore.yml --extra-vars  allspark_restore_dry_run=true", 
            "title": "Ansible Restore playbook"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting\n\n\nInstall\n\n\n\n\n\n\nTypeError\n:\n \ncreate_host_config\n()\n \ngot\n \nan\n \nunexpected\n \nkeyword\n \nargument\n \ninit\n (using verbose ansible mode)\n\n\nThe issue is caused by incompatible \ndocker-py\n and \ndocker\n python libraries versions. See \nThis ansible issue\n for more details.\nAn easy fix being to downgrade your ansible install to \nv2.5.5\n.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#install", 
            "text": "TypeError :   create_host_config ()   got   an   unexpected   keyword   argument   init  (using verbose ansible mode)  The issue is caused by incompatible  docker-py  and  docker  python libraries versions. See  This ansible issue  for more details.\nAn easy fix being to downgrade your ansible install to  v2.5.5 .", 
            "title": "Install"
        }
    ]
}